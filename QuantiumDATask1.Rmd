---
title: "Quantium DA Task1"
author: "Santosh Reddy Edulapalle"
date: "2023-02-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Load required libraries
```{r}
library(data.table)
library(ggplot2)
library(ggmosaic)
library(readr)
library(stringr)
library(dplyr)
library(arules)
library(arulesViz)
```

#### Loading the dataset

```{r}
#install.packages('readxl')
library('readxl')
trans <- read_excel('QVI_transaction_data.xlsx')
```

## Exploratory data analysis
The first step in any analysis is to first understand the data. Let's take a look
at each of the datasets provided.

### Examine transaction data

```{r}
#showing head ( top 10 rows)
head(trans)
```
```{r}
#showing summary
summary(trans)
```
```{r}
#showing high level structure
str(trans)
```
#### Convert DATE column to a date format
We can see that the DATE type is DOUBLE We need to convert it to DATE type
CSV and Excel integer dates begin on 30 Dec 1899
```{r}
typeof(trans$DATE)
trans$DATE <- as.Date(trans$DATE,origin = '1899-12-30')
typeof(trans$DATE)
#examine structure
str(trans)
```

#### Examine PROD_NAME
Since PROD_NAME is a name given to individual object, we will factorise it and make them into groups.

```{r}
trans$PROD_NAME_FACTOR <- factor(trans$PROD_NAME)
summary(trans$PROD_NAME_FACTOR)

```

```{r}
summary(trans)
str(trans)
```

#### Text analysis
### Examine product words in PROD_NAME

```{r}
library(data.table)
productWords <- data.table(unlist(strsplit(unique(trans$PROD_NAME), " ")))
setnames(productWords, 'words')
#productWords
```

### Removing words that contain numerical

```{r}
numerical.validation <- grepl('[1-9]',productWords[,words])
productWords <- productWords[numerical.validation==FALSE]
#productWords
```

### Removing words that contain special character '&'

```{r}
scAnd.validation <- grepl('&',productWords[,words])
productWords <- productWords[scAnd.validation==FALSE]
```

### Removing words that contain special character '/'

```{r}
sc.validation <- grepl('/',productWords[,words])
productWords <- productWords[sc.validation==FALSE]
```

### Counting frequencies

```{r}
#factorising words
productWords <- factor(productWords$words)
```

summary

```{r}
summary(productWords)
```
```{r}
trans <- data.table(trans)
```

#### Remove salsa products
```{r}
# Remove salsa products
trans[, SALSA := grepl("salsa", tolower(PROD_NAME))]
trans <- trans[SALSA == FALSE, ][, SALSA := NULL]
```

#### Summarise the data to check for nulls and possible outliers

```{r}
summary(trans)
```

### Checking for outliers
By seeing summary of data, we can see that the maximum value of PROD_QTY is more that (3rd quartile + 1.5*IQR)


Lets confirm this with a boxplot

```{r}
boxplot(trans$PROD_QTY)
```

Yes we can confirm existance of outliers.

```{r}
library(dplyr)
filter(trans,trans$PROD_QTY==200)
```

We have 2 records where the PROD_QTY is 200. Both are made by same customer 226000.
Let's see if he has any other transactions

```{r}
filter(trans,trans$LYLTY_CARD_NBR==226000)

```

It looks like this customer has only had the two transactions over the year and is
not an ordinary retail customer. The customer might be buying chips for commercial
purposes instead. We'll remove this loyalty card number from further analysis.

#### Filter out the customer based on the loyalty card number
Removing customer - 226000 from further analysis

```{r}
#trans[,trans$LYLTY_CARD_NBR != 226000]
trans <- trans[trans[,trans$LYLTY_CARD_NBR != 226000]]
```

#### Re-examine transaction data


```{r}
summary(trans)
```

```{r}
boxplot(trans$PROD_QTY)
```

#### Count the number of transactions by date
Let us factorise the dates

```{r}
trans$newDATE <- factor(trans$DATE)
```

Summary
```{r}
summary(trans)
str(trans)
```

There are 364 unique dates where transaction happened.
We will create a new column with dates from min to max i.e., 2018-07-01 to 2019-06-30
and then join this with trans to find that missing date.

```{r}
model.date <- seq(as.Date("2018-07-01"),as.Date("2019-06-30"),by = 'day')
model.date <- data.table(model.date)
setnames(model.date,'DATE')
#colnames(model.date) <- c('Date')
```

joining

```{r}

trans <- full_join(trans,model.date,by = c('DATE'))
```

summary

```{r}
summary(trans)
```

finding the date

```{r}

filter(trans,is.na(trans$STORE_NBR) == TRUE)
```

We can see that the date 2018-12-25 is missing.


#### Count the number of transactions by date

```{r}
transactions_per_date <- trans[, as.Date(trans$DATE, format = "%Y-%m-%d")]
transactions_per_date <- table(transactions_per_date)
transactions_per_date <- data.table(transactions_per_date)
```


#### Plot transactions over time


```{r}
library(ggplot2)
#### Setting plot themes to format graphs
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5))
#### Plot transactions over time
ggplot(transactions_per_date, aes(x = as.Date(transactions_per_date), y = N)) +
 geom_line() +
 labs(x = "Day", y = "Number of transactions", title = "Transactions over time") +
 scale_x_date(breaks = "1 month") +
 theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

#### Filter to December and look at individual days
We can see some anomaly in December.
Creating December chart to further investigate.

```{r}
x = subset(transactions_per_date, format.Date(transactions_per_date,"%m")=="12")
ggplot(x, aes(x = as.Date(transactions_per_date), y = N)) +
 geom_line() +
 labs(x = "Day", y = "Number of transactions", title = "Transactions over time") +
 scale_x_date(breaks = "1 day") +
 theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

We can see that on Dec 25 we do not have any transaction. Because it was a missing value.
Sales got increased until Christmas day and on Christmas day shops were closed.

```{r}
# removing the Christmas day
trans <- subset(trans,trans$DATE != '2018-12-25')
```

## Feature Engineering
#### Creating new features: Pack size


```{r Create pack size}
library(readr)
# creating new Pack size feature in trans by parasing our numbers from product names.
trans[, PACK_SIZE := parse_number(PROD_NAME)]
# Always check your output
# Let's check if the pack sizes look sensible
#.N is a spl variable in data.table used to represent # of observations in a group along with by = pack_size
trans[, .N, PACK_SIZE][order(PACK_SIZE)]
```

The largest size is 380g and the smallest size is 70g - seems sensible!

#### Histogram of Pack size.

```{r}
x1 <- trans$PACK_SIZE
x1 <- table(x1)
x1<- data.table(x1)
colnames(x1) <- c('Pack_size','Transactions')

barplot(height = x1$Transactions,
        names.arg = x1$Pack_size,
        main="Histogram of Pack_size to Transactions",
        xlab = "Pack size", 
        ylab= "# Transactions")
```

#### Creating new features: Brand_name

```{r}
#Here we are parsing the first word of the sentence using word() from stringr
trans$Brand_name <- word(trans$PROD_NAME, 1)

#checking brands results
trans[, .N, Brand_name][order(Brand_name)]
```

#### Clean brand names
Some of the brand names look like they are of the same brands - such as RED and
RRD, NCC and Natural Chip Co, Smith and Smiths, infuzions and infzns, Snbts and Sunbites, WW and Woolworths, Dorito and Doritos, Grain and GrnWves
Let's combine these together.

```{r}
#clean brand names
trans[Brand_name == "Red", Brand_name := "RRD"]
trans[Brand_name == "Dorito", Brand_name := "Doritos"]
trans[Brand_name == "GrnWves", Brand_name := "Grain Waves"]
trans[Brand_name == "Grain", Brand_name := "Grain Waves"]
trans[Brand_name == "Natural", Brand_name := "NCC"]
trans[Brand_name == "Smith", Brand_name := "Smiths"]
trans[Brand_name == "Infzns", Brand_name := "Infuzions"]
trans[Brand_name == "Snbts", Brand_name := "Sunbites"]
trans[Brand_name == "Woolworths", Brand_name := "WW"]

```

```{r}
#checking brands results
trans[, .N, Brand_name][order(Brand_name)]
```



#### Loading dataset

```{r load customer data}
cust = read.csv('QVI_purchase_behaviour.csv')
```

### Examining customer data
```{r}
summary(cust)
str(cust)
```

```{r fig.width=10}
#distribution of lifestage and premium_customer
ggplot(data = cust,aes(x = LIFESTAGE))+geom_histogram(stat = "count")
ggplot(data = cust,aes(x = PREMIUM_CUSTOMER))+geom_histogram(stat = "count")
```

#### Merge transaction data to customer data

```{r }
#### Merge transaction data to customer data
# all.x = T implies full left join
df <- merge(trans, cust, all.x = TRUE)

#df$LYLTY_CARD_NBR <- as.factor(df$LYLTY_CARD_NBR)
```

As the number of rows in `data` is the same as that of `transactionData`, we can be
sure that no duplicates were created. This is because we created `data` by setting
`all.x = TRUE` (in other words, a left join) which means take all the rows in
`transactionData` and find rows with matching values in shared columns and then
joining the details in these rows to the `x` or the first mentioned table.

Checking if any transactions did not have a matched customer.

```{r Check for missing customer details}
df <- df[!DATE == '2018-12-25']
```

#### Saving dataset
```{r Code to save dataset as a csv}
filePath <- "/Users/santosh/Documents/QuantiumDA/quantiumDA/"
fwrite(df, paste0(filePath,"QVI_data.csv"))
```


Data exploration is now complete!

## Data analysis on customer segments 

#### Total sales by LIFESTAGE and PREMIUM_CUSTOMER

```{r fig.width = 12,fig.align = 'center'}
ggplot(data = df,aes(x = LIFESTAGE,y = TOT_SALES, fill = PREMIUM_CUSTOMER)) + 
  geom_bar(stat = 'identity', position = "dodge") 
```


Sales are coming mainly from Budget - older families, Mainstream - young
singles/couples, and Mainstream - retirees

same output another method ( creating separate df of the required data)

```{r}

sales_summary <- df %>%
  group_by(LIFESTAGE, PREMIUM_CUSTOMER) %>%
  summarise(total_sales = sum(TOT_SALES))

```

```{r}

ggplot(sales_summary, aes(x = LIFESTAGE, y = total_sales, fill = PREMIUM_CUSTOMER)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Total Sales by LIFESTAGE and PREMIUM_CUSTOMER") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


#### Number of customers by LIFESTAGE and PREMIUM_CUSTOMER

```{r}

cust_summary <- df %>% group_by(LIFESTAGE, PREMIUM_CUSTOMER) %>%
  summarise(cust_count = n())

#n() is a function in dplyr that counts the number of observations in a group.

```

```{r}

ggplot(cust_summary, aes(x = LIFESTAGE, y = cust_count, fill = PREMIUM_CUSTOMER)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of Customers by LIFESTAGE and PREMIUM_CUSTOMER") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

There are more Mainstream - young singles/couples and Mainstream - retirees who buy
chips. This contributes to there being more sales to these customer segments but
this is not a major driver for the Budget - Older families segment.

Higher sales may also be driven by more units of chips being bought per customer.
Let's have a look at this next.

#### Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER

```{r}
avg_cust <- df %>% group_by(LIFESTAGE , PREMIUM_CUSTOMER ) %>% 
  summarize(avg_cust_count = mean(PROD_QTY))
```

```{r fig.width = 10, fig.align = "center"}
#### Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER
ggplot(data = avg_cust,aes(x = LIFESTAGE,y = avg_cust_count,fill = PREMIUM_CUSTOMER)) +   geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Older families and young families in general buy more chips per customer.

Let's also investigate the average price per unit chips bought for each customer segment as this is also a driver of total sales.

#### Average price per unit by LIFESTAGE and PREMIUM_CUSTOMER
```{r}
avg_ppu <- df %>% group_by(LIFESTAGE , PREMIUM_CUSTOMER) %>%
  summarise(avg_price = (sum(TOT_SALES)/sum(PROD_QTY)))
```



```{r fig.width = 10, fig.align = "center"}
#### Average price per unit by LIFESTAGE and PREMIUM_CUSTOMER
ggplot(data = avg_ppu,aes(x = LIFESTAGE,y = avg_price,fill = PREMIUM_CUSTOMER)) +   geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Average price per unit by LIFESTAGE and PREMIUM_CUSTOMER") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Mainstream midage and young singles and couples are more willing to pay more per
packet of chips compared to their budget and premium counterparts. This may be due
to premium shoppers being more likely to buy healthy snacks and when they buy
chips, this is mainly for entertainment purposes rather than their own consumption.
This is also supported by there being fewer premium midage and young singles and
couples buying chips compared to their mainstream counterparts.

#### Perform an independent t-test

Here we are performing t-test between Mainstream vs (Premium or Budget) for Midage and young - singles and couples.

```{r t-test: mainstream vs premium}
#### Perform an independent t-test between mainstream vs premium and budget midage and young singles and couples
# Over to you! Perform a t-test to see if the difference is significant.
main_premium <- subset(avg_ppu, PREMIUM_CUSTOMER %in% c("Mainstream", "Premium")&LIFESTAGE %in% c("YOUNG SINGLES/COUPLES","MIDAGE SINGLES/COUPLES"))

t.test(avg_price~PREMIUM_CUSTOMER, data = main_premium)
```

```{r t-test: mainstream vs budget}
#### Perform an independent t-test between mainstream vs premium and budget midage and young singles and couples
# Over to you! Perform a t-test to see if the difference is significant.
main_budget <- subset(avg_ppu, PREMIUM_CUSTOMER %in% c("Mainstream","Budget")&LIFESTAGE %in% c("YOUNG SINGLES/COUPLES","MIDAGE SINGLES/COUPLES"))

t.test(avg_price~PREMIUM_CUSTOMER, data = main_budget)
```

Both the t-test results suggest that p-value is less than $\alpha = 0.05$ and we accept alternate hypothesis that there is some difference between mean between the groups mainstream vs premium or budget.

the unit price for mainstream, young and mid-age singles and couples are significantly higher than that of budget or premium, young and midage singles and couples.


#### Deep dive into Mainstream, young singles/couples 

We might want to target customer segments that contribute the most to sales to
retain them or further increase sales. Let's look at Mainstream - young
singles/couples. For instance, let's find out if they tend to buy a particular
brand of chips.



```{r fig.align = "center"}


#get brand baskets
Baskets <- df %>% group_by(PREMIUM_CUSTOMER == "Mainstream" & LIFESTAGE == "YOUNG SINGLES/COUPLES") %>%
  summarise(basket = list(Brand_name))

str(Baskets)

#compute transactions
transx <- as(Baskets$basket,"transactions")

# Perform affinity analysis using Apriori algorithm
rules <- apriori(transx, parameter = list(supp = 0.5, conf = 0.9, target = "rules"))

#summary of rules
summary(rules)

#Inspect rules with the highest lift.
inspect(head(sort(rules, by = "lift")))

# Visualize the rules
plot(rules, method = "graph")

```

Here we are perfoming apriori analysis to find out which brands are associated with each other and most importantly which brand does Mainstream and Young couples / singles prefer most. 
here we are using a support of 0.5 and confidence of 0.9

As we can see from the results brands like Burger, Tyrrells, Twisties, Tostitos, Thins, are among the top 5 buyed brands in these two customer segements. 



Let's also find out if our target segment tends to buy larger packs of chips.
```{r fig.align = "center"}
#### Preferred pack size compared to the rest of the population
# Over to you! Do the same for pack size.
```


